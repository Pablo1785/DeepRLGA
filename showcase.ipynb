{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from deep_rl_ga.ga_env import GeneticAlgorithmEnv\n",
    "from deep_rl_ga.agent import Agent\n",
    "from deep_rl_ga.memory import Experience, ReplayMemory, extract_tensors\n",
    "from deep_rl_ga.strategy import EpsilonGreedyStrategy\n",
    "from deep_rl_ga.network import DQN\n",
    "from deep_rl_ga.qvalues import QValues\n",
    "\n",
    "from deap import benchmarks\n",
    "from deap import tools\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "def plot(values, moving_avg_period, is_ipython=True):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(0.001)\n",
    "    print('Episode', len(values), '\\n', moving_avg_period, 'episode moving avg:', moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period - 1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbilko/miniconda3/envs/PP/lib/python3.9/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/pbilko/miniconda3/envs/PP/lib/python3.9/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 78>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m timestep \u001B[38;5;129;01min\u001B[39;00m count():\n\u001B[1;32m     83\u001B[0m     action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mselect_action(state, policy_net)\n\u001B[0;32m---> 84\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[43mem\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m em\u001B[38;5;241m.\u001B[39mget_state()\n\u001B[1;32m     86\u001B[0m     memory\u001B[38;5;241m.\u001B[39mpush(Experience(state, action, next_state, reward))\n",
      "File \u001B[0;32m~/STUDIA/Neuronowa Adaptacja Operatorów Wariacyjnych/deep_rl_ga/ga_env.py:119\u001B[0m, in \u001B[0;36mGeneticAlgorithmEnv.take_action\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtake_action\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m    :param action: Tensor with a single value - index of chosen action\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m    :return:\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_state, reward, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor([reward])\n",
      "File \u001B[0;32m~/STUDIA/Neuronowa Adaptacja Operatorów Wariacyjnych/deep_rl_ga/ga_env.py:169\u001B[0m, in \u001B[0;36mGeneticAlgorithmEnv.step\u001B[0;34m(self, action_idx)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;66;03m# Evaluate\u001B[39;00m\n\u001B[1;32m    165\u001B[0m fits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoolbox\u001B[38;5;241m.\u001B[39mevaluate,\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpopulation\n\u001B[1;32m    168\u001B[0m     )\n\u001B[0;32m--> 169\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fit, ind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m    170\u001B[0m         fits,\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpopulation\n\u001B[1;32m    172\u001B[0m         ):\n\u001B[1;32m    173\u001B[0m     ind\u001B[38;5;241m.\u001B[39mfitness\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;241m=\u001B[39m fit\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevals_left \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpopulation\n\u001B[1;32m    176\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/PP/lib/python3.9/site-packages/deap/benchmarks/__init__.py:239\u001B[0m, in \u001B[0;36mrastrigin\u001B[0;34m(individual)\u001B[0m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrastrigin\u001B[39m(individual):\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;124;03m\"\"\"Rastrigin test objective function.\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    .. list-table:: \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;124;03m       :width: 67 %\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m     \n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(individual) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28msum\u001B[39m(gene \u001B[38;5;241m*\u001B[39m gene \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m*\u001B[39m \\\n\u001B[1;32m    240\u001B[0m                         cos(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m pi \u001B[38;5;241m*\u001B[39m gene) \u001B[38;5;28;01mfor\u001B[39;00m gene \u001B[38;5;129;01min\u001B[39;00m individual),\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Genetic algorithm params\n",
    "IND_SIZE = 3\n",
    "LOW_BOUND = -5.12\n",
    "UP_BOUND = 5.12\n",
    "FITNESS_FUNCTION = benchmarks.rastrigin\n",
    "\n",
    "# Crossover + Mutation params\n",
    "MATING_RATE = 0.3\n",
    "INDIVIDUAL_MUTATION_RATE = 0.3\n",
    "\n",
    "# Selection params\n",
    "TOURNAMENT_SIZE = 3\n",
    "TOP_BEST_SIZE = 10\n",
    "\n",
    "INITIAL_POPULATION_SIZE = 150\n",
    "MAX_EVALS = 10_000\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "random.seed(\n",
    "    RANDOM_SEED\n",
    "    )\n",
    "np.random.seed(\n",
    "    RANDOM_SEED\n",
    "    )\n",
    "\n",
    "ACTIONS_SEL = [\n",
    "    {'function': tools.selTournament, 'tournsize': TOURNAMENT_SIZE},\n",
    "    {'function': tools.selBest, 'k': TOP_BEST_SIZE},\n",
    "]\n",
    "\n",
    "ACTIONS_CX = [\n",
    "    {'function': tools.cxBlend, 'alpha': UP_BOUND},\n",
    "    {'function': tools.cxTwoPoint},\n",
    "]\n",
    "\n",
    "ACTIONS_MU = [\n",
    "    {'function': tools.mutGaussian, 'mu': 0, 'sigma': 1, 'indpb': INDIVIDUAL_MUTATION_RATE},\n",
    "    {'function': tools.mutShuffleIndexes, 'indpb': INDIVIDUAL_MUTATION_RATE},\n",
    "]\n",
    "\n",
    "\n",
    "# Neural network params\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100_000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "curr_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "em = GeneticAlgorithmEnv(\n",
    "    num_dims=IND_SIZE,\n",
    "    low_bound=LOW_BOUND,\n",
    "    up_bound=UP_BOUND,\n",
    "    fitness_fn=benchmarks.rastrigin,\n",
    "    max_evals=MAX_EVALS,\n",
    "    initial_population_size=INITIAL_POPULATION_SIZE,\n",
    "    actions_sel=ACTIONS_SEL,\n",
    "    actions_cx=ACTIONS_CX,\n",
    "    actions_mu=ACTIONS_MU,\n",
    "    device=curr_device,\n",
    ")\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), curr_device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(em.get_num_state_features(), em.num_actions_available()).to(curr_device)\n",
    "target_net = DQN(em.get_num_state_features(), em.num_actions_available()).to(curr_device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences, curr_device)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states, curr_device)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if em.done:\n",
    "            # TODO: This should be a value that we want to track across episodes, e.x. number of generations before global optimum was found\n",
    "            episode_rewards.append(reward)\n",
    "            plot(episode_rewards, 100)\n",
    "            break\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # TODO: This should be a value that we want to track across episodes, e.x. number of generations before global optimum was found\n",
    "    if get_moving_average(100, episode_rewards)[-1] <= (MAX_EVALS // INITIAL_POPULATION_SIZE) // 2:\n",
    "        break\n",
    "\n",
    "em.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}