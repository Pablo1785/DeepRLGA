{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from deep_rl_ga.ga_env import GeneticAlgorithmEnv\n",
    "from deep_rl_ga.agent import Agent\n",
    "from deep_rl_ga.memory import Experience, ReplayMemory, extract_tensors\n",
    "from deep_rl_ga.strategy import EpsilonGreedyStrategy\n",
    "from deep_rl_ga.network import DQN\n",
    "from deep_rl_ga.qvalues import QValues\n",
    "\n",
    "from deap import benchmarks\n",
    "from deap import tools\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "def plot(values, moving_avg_period, is_ipython=True):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Best fitness')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(0.001)\n",
    "    print('Episode', len(values), '\\n', moving_avg_period, 'episode moving avg:', moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period - 1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotrswiderski/.conda/envs/DeepRLGA/lib/python3.9/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/Users/piotrswiderski/.conda/envs/DeepRLGA/lib/python3.9/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# Genetic algorithm params\n",
    "IND_SIZE = 3\n",
    "LOW_BOUND = -5.12\n",
    "UP_BOUND = 5.12\n",
    "FITNESS_FUNCTION = benchmarks.rastrigin\n",
    "\n",
    "# Crossover + Mutation params\n",
    "MATING_RATE = 0.3\n",
    "INDIVIDUAL_MUTATION_RATE = 0.3\n",
    "\n",
    "# Selection params\n",
    "TOURNAMENT_SIZE = 3\n",
    "TOP_BEST_SIZE = 10\n",
    "\n",
    "INITIAL_POPULATION_SIZE = 150\n",
    "MAX_EVALS = 10_000\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "random.seed(\n",
    "    RANDOM_SEED\n",
    "    )\n",
    "np.random.seed(\n",
    "    RANDOM_SEED\n",
    "    )\n",
    "\n",
    "ACTIONS_SEL = [\n",
    "    {'function': tools.selTournament, 'tournsize': TOURNAMENT_SIZE},\n",
    "    {'function': tools.selBest, 'k': TOP_BEST_SIZE},\n",
    "]\n",
    "\n",
    "ACTIONS_CX = [\n",
    "    {'function': tools.cxBlend, 'alpha': UP_BOUND},\n",
    "    {'function': tools.cxTwoPoint},\n",
    "]\n",
    "\n",
    "ACTIONS_MU = [\n",
    "    {'function': tools.mutGaussian, 'mu': 0, 'sigma': 1, 'indpb': INDIVIDUAL_MUTATION_RATE},\n",
    "    {'function': tools.mutShuffleIndexes, 'indpb': INDIVIDUAL_MUTATION_RATE},\n",
    "]\n",
    "\n",
    "\n",
    "# Neural network params\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100_000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "curr_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "em = GeneticAlgorithmEnv(\n",
    "    num_dims=IND_SIZE,\n",
    "    low_bound=LOW_BOUND,\n",
    "    up_bound=UP_BOUND,\n",
    "    fitness_fn=benchmarks.rastrigin,\n",
    "    max_evals=MAX_EVALS,\n",
    "    initial_population_size=INITIAL_POPULATION_SIZE,\n",
    "    actions_sel=ACTIONS_SEL,\n",
    "    actions_cx=ACTIONS_CX,\n",
    "    actions_mu=ACTIONS_MU,\n",
    "    device=curr_device,\n",
    ")\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), curr_device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(em.get_num_state_features(), em.num_actions_available()).to(curr_device)\n",
    "target_net = DQN(em.get_num_state_features(), em.num_actions_available()).to(curr_device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "episode_best_fitnesses = []\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences, curr_device)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states, curr_device)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if em.done:\n",
    "            # TODO: This should be a value that we want to track across episodes, e.x. number of generations before global optimum was found\n",
    "            episode_best_fitnesses.append(em.hof[0].fitness.values[0])\n",
    "            plot(episode_best_fitnesses, 100)\n",
    "            break\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # TODO: This should be a value that we want to track across episodes, e.x. number of generations before global optimum was found\n",
    "    # if get_moving_average(100, episode_best_fitnesses)[-1] <= 0.0001:\n",
    "    #     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}